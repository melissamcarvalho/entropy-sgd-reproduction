* Robust Generalization Measures Analysis

How to use it?

* Class Experiment runs the experiment and is similar to my training process
    - Do not need to be used

* Import that is necessary: `from .measures import get_all_measures`

When to calculate?

get_all_measures is calculated with a training flag because the data_loader
is a concatenation of two types of data

Inputs:

model: ExperimentBaseModel
    - Inherit Class from nn.Module
        self.model = MyPreferedArchtecture
init_model: ExperimentBaseModel
    - Inherit Class from nn.Module
    - Initial Model without tuning
        self.init_model = deepcopy(self.model)
dataloader: DataLoader
    CIFAR10 (60000: 50000/10000)
    data_loader = [self.train_eval_loader, self.test_loader][dataset_subset_type]
    dataset_subset_type: Enum that is zero for train and 1 for test
    self.train_eval_loader = DataLoader(train, batch_size=5000, shuffle=False, num_workers=0)
        - loads the training set on a large batch. For efficiency?
    self.test_loader = DataLoader(test, batch_size=5000, shuffle=False, num_workers=0)
        - loads half of the test set
    dataset_subset_type?
acc: Accuracy on the complete train (validation) set or accuracy on the complete validation (test) set
    - On the code: The complete train_eval_loader and the complete test_eval_loader
    - Get the accuracy for all the batches on evaluate_cross_entropy
    - acc in all data_loader
seed: int
    -

The generalization gap

Building two different EvaluationMetrics objects, we can generate the generalization gap

EvaluationMetrics objects

  acc: Sum the correct values over all the batches and at the end, divide by the length of the dataset
  avg_loss: Sum the loss over the batches and at the end divide by length pf the dataset
  num_correct: Sum of correct values over all the dataset (sum over the batches)
  num_to_evaluate_on: Size of the dataset
  all_complexities: Dict[ComplexityType, float]


Conclusion of implementations:

1. Evaluate: Return the object EvaluationMetrics

2. Evaluate Cross Entropy (Used by Evaluate to generate metrics)

3. Position of Evaluate: Get the elements from the EvaluationMetrics
    and generates the generalization gap

Use Weights and Biases Logger <3




